---
layout: page
title: Available theses and projects
permalink: /theses/
---

The following is a list of experimental research topics currently available for master theses and master/Ph.D. projects. 

Please [contact me](mailto:giulio.rossolini@santannapisa.it) to get more details and schedule a meeting. 
If you would like to address other topics close to safe and secure aspects of AI, feel free to [contact me](mailto:giulio.rossolini@santannapisa.it) too!

<em>General requirements (not hard) </em>: a basic knowledge of deep neural networks and computer vision tasks; a minimum level of programming skills with python and classic AI libraries (e.g., Torch, Tensorflow, SciPy).
<br>

<hr>

### 1 - Improving <b>Deep Steganography</b> via Generative Adversarial Models


<table>
<tr>
<p style='font-size:90%'>
In cyber-security and data engineering, steganography is the art of hiding data (images, text, etc.) into other data. It allows sending secret messages between multiple users through apparently trustful public data. In recent years, classic steganography approaches have been outperformed by the advent of Deep Learning, opening new research directions in the AI security landscape.
<br><br>
The goal of the project consists of leveraging Generative Adversarial Models (GANs) to enlarge the amount of secret data hideable in a public message.
</p>
</tr>
<tr style="padding:20px;width:40%;vertical-align:middle; horizontal-align:middle">
<p style="text-align:center; margin-top: 0px; margin-bottom: 0px">
  <img src="{{ site.baseurl }}/images/thesis/steganography.webp" width="600" height="150">
</p>
</tr>
</table>



<hr>

### 2 - Addressing Adversarial Perturbation through a <b>Fourier Analysis</b>
<table cellpadding="0" cellspacing="0" width="100%" border-collapse="collapse">
<tr>
    <td width="60%" valign="middle">
    <p style='font-size:90%'>
   Adversarial perturbation is a famous trend of robust AI, which consists of malicious and imperceptible input noise intentionally injected to fool the DNN prediction. 
   <br>
   While many studies have addressed such a problem in the image domain, just a few have analyzed it in the frequency domain.
  <br><br><br>
  This project aims at deepening the analysis of adversarial perturbations through the frequency domain and thus address novel defense strategies to improve the robustness of a neural model.
    </p>
    </td>
    <td style="padding:20px;width:40%;vertical-align:middle; horizontal-align:middle">
        <p style="text-align:center; margin-top: 0px; margin-bottom: 0px">
          <img src="{{ site.baseurl }}/images/thesis/frequency_analysis.jpg" width="80%" height="100%">  
        </p>  
    </td>
</tr>
</table>


<hr>

### 3 -  <b>Poisoning Attacks</b> for Computer Vision Models
<table cellpadding="0" cellspacing="0" width="100%" border-collapse="collapse">
<tr>
    <td width="60%" valign="middle">
    <p style='font-size:90%'>
    The high performance achieved by current deep neural networks mainly relies on the availability and quality of annotated training data. However, the correctness and fairness of the training set can be compromised by so-called poisoning attacks where malicious users inject fake training samples or alter the available ones to corrupt the trained model, making dangerous and unexpected behaviors. 
    <br>
    </p>
    </td>
    <td style="padding:20px;width:40%;vertical-align:middle; horizontal-align:middle">
        <p style="text-align:center; margin-top: 0px; margin-bottom: 0px">
          <img src="{{ site.baseurl }}/images/thesis/poisoning.webp" width="100%" height="100%">  
        </p>  
    </td>
</tr>
</table>

<table>
<tr>
  <td>
    This work aims at deepening the analysis of poisoning attacks against large-scale datasets and proposing novel attack strategies to deceive the spatial robustness of computer vision models (e.g., semantic segmentation and object detection).
    </td>
</tr>
</table>


<hr>

### 4 -  <b>Autonomous Driving Simulators </b> for Adversarial Robustness.
<table>
<tr>
<p style='font-size:90%'>
Addressing the robustness of computer vision tasks in autonomous driving applications is a challenging and expensive task, due to the hard testing processes and setup time. 
To this end, recent studies have shown that Autonomous Driving Simulator can represent a valuable and computationally-affordable solution since the annotations and testing scenarios can be generated automatically by the available software tools (e.g., CARLA).
<br><br>
This work addresses open problems of current Autonomous Driving Simulators used to evaluate the model robustness, such as domain shift issues and testing scenarios.
Potentially, the work can also unlock new research directions for investigating new robust domain adaptation strategies.
<br><br>
<em>Note</em>. Basic knowledge of CARLA and Unreal Engine environments is recommended.
</p>
</tr>
<tr style="padding:20px;width:40%;vertical-align:middle; horizontal-align:middle">
        <p style="text-align:center; margin-top: 0px; margin-bottom: 0px">
          <img src="{{ site.baseurl }}/images/thesis/carla.png" width="90%" height="20%">  
        </p>  
</tr>
</table>

<hr> 


### 5 -  Unsupervised Domain Adaptation for Railway Segmentation
<table>
<tr>
<p style='font-size:90%'>
Domain adaptation techniques help improve the accuracy of a neural model whenever there is a scarce availability of real-world annotated samples, which is particularly true for the railway domain.
<br><br>
The project aims at extending and improving previous domain adaptation techniques to railway scenarios, thus enabling an accurate rails segmentation relying only on synthetic annotated samples.
</p>
</tr>
<tr style="padding:20px;width:40%;vertical-align:middle; horizontal-align:middle">
        <p style="text-align:center; margin-top: 0px; margin-bottom: 0px">
          <img src="{{ site.baseurl }}/images/thesis/railway.png" width="70%" height="110px">  
        </p>  
</tr>
</table>

<hr> 

### 6 - [Assigned] <del>Improving the <b>Coverage Analysis of DNNs</b> via Predictable Models</del>

<hr>

### Contact
[giulio.rossolini@santannapisa.it](mailto:giulio.rossolini@santannapisa.it)

#### Last update
Feb 20, 2023